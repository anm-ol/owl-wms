# configs/tekken_rft_train.yml
model:
  model_id: tekken_rft
  sample_size: [14, 23]     # Latent H, W from your VAE
  channels: 128             # Latent channels from your VAE
  n_actions: 256            # For the action embedding layer
  n_layers: 12
  n_heads: 16
  d_model: 768
  # tokens_per_frame will be calculated inside the model as 14*23=322
  n_frames: 1000              # This is the window length
  cfg_prob: 0.0
  causal: true
  uncond: false
  backbone: dit
  has_audio: false

  local_window: 8
  global_window: 8

train:
  trainer_id: tekken_rft
  data_id: tekken_multi
  data_kwargs:
    window_length: 8
    root_dir: "preproccessing/cached_data_v3/train"

  sample_data_id: tekken_multi
  sample_data_kwargs:
    window_length: 8  # size of single ground truth start tokens
    root_dir: "preproccessing/cached_data_v3/val"

  target_batch_size: 32
  batch_size: 8 # Adjust based on your GPU memory
  compile: false

  epochs: 50000
  opt: Muon
  opt_kwargs:
    lr: 1.0e-3
    momentum: 0.95
    adamw_lr: 1.0e-4
    adamw_wd: 1.0e-4
    adamw_eps: 1.0e-15
    adamw_betas: [0.9, 0.95]
    adamw_keys: [core.proj_in, core.proj_out.proj, core.t_embed, core.action_embed, gate, adaln]

  checkpoint_dir: checkpoints/tekken_dit_v2.5
  output_path: checkpoints/tekken_dit_v2.5_ema
  resume_ckpt: null #checkpoints/tekken_dit_v2/step_24000.pt # Path to your last checkpoint

  sample_interval: 500
  save_interval: 1000
  n_samples: 4
  eval_sample_dir: samples/tekken_dit_v2

  sampler_id: t3_caching
  sampler_kwargs:
    n_steps: 16
    # window_length: 8
    cfg_scale: 1.0
    num_frames: 16
    noise_prev: 0.2
    only_return_generated: false

  # VAE settings for decoding samples
  vae_id: ltx # Not a built-in HF VAE
  vae_cfg_path: null # Path to your VAE config
  vae_ckpt_path: "preproccessing/checkpoints/LTXV/vae" # Path to your VAE checkpoint
  vae_scale: 1.0 # Adjust if your VAE has a scaling factor
  vae_batch_size: 8

wandb:
  name: d-fusion
  project: OWL
  run_name: tekken-rft-v2.5