# configs/tekken_rft_train.yml
model:
  model_id: tekken_rft_v2
  sample_size: [14, 23]     # Latent H, W from your VAE
  channels: 128             # Latent channels from your VAE
  n_buttons: 8            # For the action embedding layer
  n_layers: 16 #16
  n_heads: 16
  d_model: 2048 #1536
  # tokens_per_frame will be calculated inside the model as 14*23=322
  tokens_per_frame: 322
  action_tokens_per_frame: null
  image_tokens_per_frame: null
  # rope_impl: "tekken"
  n_frames: 1000              # This is the window length
  cfg_prob: 0.0
  causal: true
  uncond: false
  backbone: dit
  has_audio: false

  local_window: 16
  global_window: 128

train:
  trainer_id: tekken_rft_v2
  data_id: tekken_multi
  data_kwargs:
    window_length: 128
    temporal_compression: 1
    root_dir: "/mnt/data/laplace/owl-wms/preproccessing/cached_dcae_nopose/train"

  sample_data_id: tekken_multi
  sample_data_kwargs:
    window_length: 128  # size of single ground truth start tokens
    temporal_compression: 1
    root_dir: "/mnt/data/laplace/owl-wms/preproccessing/cached_dcae_nopose/val"

  target_batch_size: 8
  batch_size: 1
  compile: true

  epochs: 50000
  opt: Muon
  opt_kwargs:
    lr: 1.0e-3
    momentum: 0.95
    adamw_lr: 1.0e-4
    adamw_wd: 1.0e-4
    adamw_eps: 1.0e-15
    adamw_betas: [0.9, 0.95]
    adamw_keys: [core.proj_in, core.proj_out.proj, core.t_embed, core.action_embed, gate, adaln]

  checkpoint_dir: checkpoints/tekken_nopose_large
  output_path: /mnt/data/laplace/owl-wms/checkpoints/tekken_nopose_large
  resume_ckpt: null # checkpoints/tekken_dit_v2_actions/step_2000.pt # Path to your last checkpoint

  sample_interval: 2500
  save_interval: 5000
  n_samples: 2 #8
  eval_sample_dir: null #samples/tekken_dit_v2_actions

  sampler_id: t3_action_caching
  sampler_kwargs:
    n_steps: 32
    # window_length: 8
    cfg_scale: 1.0
    num_frames: 192 # 16
    noise_prev: 0.2
    only_return_generated: false

  # VAE settings for decoding samples
  vae_id: null
  vae_cfg_path: "/mnt/data/laplace/owl-wms/preproccessing/checkpoints/t3_VAE_nopose_v1/config.yaml" #null # Path to your VAE config
  vae_ckpt_path: "/mnt/data/laplace/owl-wms/preproccessing/checkpoints/t3_VAE_nopose_v1/step_49000.pt" # Path to your VAE checkpoint
  vae_scale: 1.0 # Adjust if your VAE has a scaling factor
  vae_batch_size: 4

wandb:
  name: d-fusion
  project: OWL
  run_name: tekken-nopose-wm